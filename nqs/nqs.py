#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import sys
import warnings
from abc import abstractmethod
from functools import partial
from multiprocessing import Lock, RLock
from threading import RLock as TRLock

import numpy as np
import pandas as pd
from numpy.random import default_rng
from pathos.pools import ProcessPool
from tqdm.auto import tqdm

from .models import IRBM, JAXIRBM, JAXNIRBM, NIRBM
# from .state import State
from .utils import (State, advance_PRNG_state, block, check_and_set_nchains,
                    early_stopping, generate_seed_sequence, setup_logger,
                    tune_scale_lmh_table, tune_scale_rwm_table)

warnings.filterwarnings("ignore", message="divide by zero encountered")


class NotInitialized(Exception):
    r"""Failed attempt at accessing posterior samples.
    A call to the sample method must be carried out first.
    """
    pass


class NotTrained(Exception):
    r"""Failed attempt at accessing posterior samples.
    A call to the sample method must be carried out first.
    """
    pass


class SamplingNotPerformed(Exception):
    r"""Failed attempt at accessing posterior samples.
    A call to the sample method must be carried out first.
    """
    pass


class NQS:

    def __init__(
        self,
        nparticles,
        dim,
        nhidden=1,
        interaction=False,
        mcmc_alg='rwm',
        nqs_repr='psi2',
        backend='numpy',
        log=True,
        logger_level="INFO",
        rng=None
    ):
        """Neural Network Quantum State
        """

        self._check_logger(log, logger_level)
        self._log = log

        if self._log:
            self.logger = setup_logger(self.__class__.__name__,
                                       level=logger_level)
        else:
            self.logger = None

        self._P = nparticles
        self._dim = dim
        self._mcmc_alg = mcmc_alg
        self._nhidden = nhidden
        self._nvisible = self._P * self._dim

        if rng is None:
            rng = default_rng
        self._rng = rng

        if nqs_repr == 'psi':
            factor = 1.0
        elif nqs_repr == 'psi2':
            factor = 0.5
        else:
            msg = ("The NQS can only represent the wave function itself "
                   "('psi') or the wave function amplitude ('psi2')")
            raise ValueError(msg)

        if backend == 'numpy':
            if interaction:
                self._rbm = IRBM(self._P, self._dim, factor=factor)
            else:
                self._rbm = NIRBM(factor=factor)
        elif backend == 'jax':
            if interaction:
                self._rbm = JAXIRBM(self._P, self._dim, factor=factor)
            else:
                self._rbm = JAXNIRBM(factor=factor)
        else:
            msg = "Unsupported backend, only 'numpy' or 'jax' is allowed"
            raise ValueError(msg)

        # set mcmc step
        self._set_mcmc_alg(mcmc_alg)

        if self._log:
            neuron_str = 'neurons' if self._nhidden > 1 else 'neuron'
            msg = (f"Neural Network Quantum State initialized as RBM with "
                   f"{self._nhidden} hidden {neuron_str}")
            self.logger.info(msg)

        # flags
        self._is_initialized_ = False
        self._is_trained_ = False
        self._is_tuned_ = False
        self._sampling_performed_ = False

    def _set_mcmc_alg(self, mcmc_alg):
        if mcmc_alg == 'rwm':
            self._mcmc_alg = mcmc_alg
            self._step = self._rwm_step
            self._tune_table = tune_scale_rwm_table
        elif mcmc_alg == 'lmh':
            self._mcmc_alg = mcmc_alg
            self._step = self._lmh_step
            self._tune_table = tune_scale_lmh_table
        else:
            msg = "Unsupported MCMC algorithm, only 'rwm' or 'lmh' is allowed"
            raise ValueError(msg)

    def _is_initialized(self):
        if not self._is_initialized_:
            msg = ("A call to 'init' must be made before training")
            raise NotInitialized(msg)

    def _is_trained(self):
        if not self._is_trained_:
            msg = ("A call to 'train' must be made before sampling")
            raise NotTrained(msg)

    def _sampling_performed(self):
        if not self._is_trained_:
            msg = ("A call to 'sample' must be made in order to access results")
            raise SamplingNotPerformed(msg)

    def _check_logger(self, log, logger_level):
        if not isinstance(log, bool):
            raise TypeError("'log' must be True or False")

        if not isinstance(logger_level, str):
            raise TypeError("'logger_level' must be passed as str")

    def init(
        self,
        sigma2=1.,
        scale=0.5,
        seed=None
    ):
        """
        """
        self._rbm.sigma2 = sigma2
        self._scale = scale

        rng = self._rng(seed)

        r = rng.standard_normal(size=self._nvisible)
        self._v_bias = rng.standard_normal(size=self._nvisible) * 0.01
        self._h_bias = rng.standard_normal(size=self._nhidden) * 0.01
        self._kernel = rng.standard_normal(size=(self._nvisible,
                                                 self._nhidden))
        logp = self._rbm.logprob(r, self._v_bias, self._h_bias, self._kernel)
        self._state = State(r, logp, 0, 0)

        self._is_initialized_ = True

    def train(
        self,
        max_iter=100_000,
        batch_size=1000,
        gradient_method='adam',
        eta=0.01,
        beta1=0.9,
        beta2=0.999,
        epsilon=1e-8,
        early_stop=False,  # set to True later
        rtol=1e-05,
        atol=1e-08,
        seed=None,
        mcmc_alg=None
    ):
        """
        """

        self._is_initialized()

        if mcmc_alg is not None:
            self._set_mcmc_alg(mcmc_alg)

        state = self._state
        v_bias = self._v_bias
        h_bias = self._h_bias
        kernel = self._kernel
        scale = self._scale
        # Reset n_accepted
        state = State(state.positions, state.logp, 0, state.delta)

        if self._log:
            t_range = tqdm(range(max_iter),
                           desc=f"[Training progress]",
                           position=0,
                           leave=True,
                           colour='green')
        else:
            t_range = range(max_iter)

        # Set parameters for Adam
        if gradient_method == "adam":
            t = 0
            # visible bias
            m_v_bias = np.zeros_like(v_bias)
            v_v_bias = np.zeros_like(v_bias)
            # hidden bias
            m_h_bias = np.zeros_like(h_bias)
            v_h_bias = np.zeros_like(h_bias)
            # kernel
            m_kernel = np.zeros_like(kernel)
            v_kernel = np.zeros_like(kernel)

        # Config
        did_early_stop = False
        seed_seq = generate_seed_sequence(seed, 1)[0]
        steps_before_optimize = batch_size
        energies = []
        grads_v_bias = []
        grads_h_bias = []
        grads_kernel = []

        # Training
        for i in t_range:
            state = self._step(state, v_bias, h_bias, kernel, seed_seq)
            loc_energy = self._rbm.local_energy(state.positions,
                                                v_bias,
                                                h_bias,
                                                kernel
                                                )
            gr_v_bias = self._rbm.grad_v_bias(state.positions,
                                              v_bias,
                                              h_bias,
                                              kernel
                                              )
            gr_h_bias = self._rbm.grad_h_bias(state.positions,
                                              v_bias,
                                              h_bias,
                                              kernel
                                              )
            gr_kernel = self._rbm.grad_kernel(state.positions,
                                              v_bias,
                                              h_bias,
                                              kernel
                                              )
            energies.append(loc_energy)
            grads_v_bias.append(gr_v_bias)
            grads_h_bias.append(gr_h_bias)
            grads_kernel.append(gr_kernel)

            steps_before_optimize -= 1

            if steps_before_optimize == 0:
                # Expectation values
                energies = np.array(energies)
                grads_v_bias = np.array(grads_v_bias)
                grads_h_bias = np.array(grads_h_bias)
                grads_kernel = np.array(grads_kernel)

                expval_energy = np.mean(energies)
                expval_grad_v_bias = np.mean(grads_v_bias, axis=0)
                expval_grad_h_bias = np.mean(grads_h_bias, axis=0)
                expval_grad_kernel = np.mean(grads_kernel, axis=0)
                expval_energy_v_bias = np.mean(
                    energies.reshape(batch_size, 1) * grads_v_bias, axis=0)
                expval_energy_h_bias = np.mean(
                    energies.reshape(batch_size, 1) * grads_h_bias, axis=0)
                expval_energy_kernel = np.mean(
                    energies.reshape(batch_size, 1, 1) * grads_kernel, axis=0)

                # Gradients
                final_gr_v_bias = 2 * \
                    (expval_energy_v_bias - expval_energy * expval_grad_v_bias)
                final_gr_h_bias = 2 * \
                    (expval_energy_h_bias - expval_energy * expval_grad_h_bias)
                final_gr_kernel = 2 * \
                    (expval_energy_kernel - expval_energy * expval_grad_kernel)

                # Gradient descent
                if gradient_method == "gd":
                    v_bias -= eta * final_gr_v_bias
                    h_bias -= eta * final_gr_h_bias
                    kernel -= eta * final_gr_kernel

                elif gradient_method == "adam":
                    t += 1
                    # update visible bias
                    m_v_bias = beta1 * m_v_bias + (1 - beta1) * final_gr_v_bias
                    v_v_bias = beta2 * v_v_bias + \
                        (1 - beta2) * final_gr_v_bias**2
                    m_hat_v_bias = m_v_bias / (1 - beta1**t)
                    v_hat_v_bias = v_v_bias / (1 - beta2**t)
                    v_bias -= eta * m_hat_v_bias / \
                        (np.sqrt(v_hat_v_bias) - epsilon)
                    # update hidden bias
                    m_h_bias = beta1 * m_h_bias + (1 - beta1) * final_gr_h_bias
                    v_h_bias = beta2 * v_h_bias + \
                        (1 - beta2) * final_gr_h_bias**2
                    m_hat_h_bias = m_h_bias / (1 - beta1**t)
                    v_hat_h_bias = v_h_bias / (1 - beta2**t)
                    h_bias -= eta * m_hat_h_bias / \
                        (np.sqrt(v_hat_h_bias) - epsilon)
                    # update kernel
                    m_kernel = beta1 * m_kernel + (1 - beta1) * final_gr_kernel
                    v_kernel = beta2 * v_kernel + \
                        (1 - beta2) * final_gr_kernel**2
                    m_hat_kernel = m_kernel / (1 - beta1**t)
                    v_hat_kernel = v_kernel / (1 - beta2**t)
                    kernel -= eta * m_hat_kernel / \
                        (np.sqrt(v_hat_kernel) - epsilon)

                energies = []
                grads_v_bias = []
                grads_h_bias = []
                grads_kernel = []
                steps_before_optimize = batch_size

        # early stop flag activated
        # numpy.isclose
        if did_early_stop:
            msg = ("Early stopping, training converged")
            self.logger.info(msg)
        # msg: Early stopping, training converged

        # end
        # Update shared values
        self._state = state
        self._v_bias = v_bias
        self._h_bias = h_bias
        self._kernel = kernel
        self._scale = scale
        self._is_trained_ = True

    def tune(
        self,
        tune_iter=20_000,
        tune_interval=500,
        early_stop=False,  # set to True later
        rtol=1e-05,
        atol=1e-08,
        seed=None,
        mcmc_alg=None
    ):
        """
        Tune proposal scale
        """

        self._is_initialized()
        state = self._state
        v_bias = self._v_bias
        h_bias = self._h_bias
        kernel = self._kernel
        scale = self._scale

        if mcmc_alg is not None:
            self._set_mcmc_alg(mcmc_alg)

        # Used to throw warnings if tuned alg mismatch chosen alg
        # in other procedures
        self._tuned_mcmc_alg = self._mcmc_alg

        # Config
        did_early_stop = False
        seed_seq = generate_seed_sequence(seed, 1)[0]

        # Reset n_accepted
        state = State(state.positions, state.logp, 0, state.delta)

        if self._log:
            t_range = tqdm(range(tune_iter),
                           desc=f"[Tuning progress]",
                           position=0,
                           leave=True,
                           colour='green')
        else:
            t_range = range(tune_iter)

        steps_before_tune = tune_interval

        for i in t_range:
            state = self._step(state, v_bias, h_bias, kernel, seed_seq)
            steps_before_tune -= 1

            if steps_before_tune == 0:

                # Tune proposal scale
                old_scale = scale
                accept_rate = state.n_accepted / tune_interval
                scale = self._tune_table(old_scale, accept_rate)

                # Reset
                steps_before_tune = tune_interval
                state = State(state.positions, state.logp, 0, state.delta)

        # Update shared values
        self._state = state
        self._v_bias = v_bias
        self._h_bias = h_bias
        self._kernel = kernel
        self._scale = scale
        self._is_tuned_ = True

    def sample(self, nsamples, nchains=1, burn=0, seed=None, mcmc_alg=None):
        """

        """

        # TODO: accept biases and kernel as parameters and
        # assume they are optimized if passed
        self._is_initialized()
        self._is_trained()
        state = self._state
        v_bias = self._v_bias
        h_bias = self._h_bias
        kernel = self._kernel
        scale = self._scale

        if mcmc_alg is not None:
            self._set_mcmc_alg(mcmc_alg)

        #nchains = check_and_set_nchains(nchains, self.logger)
        #seeds = generate_seed_sequence(seed, nchains)
        seed_seq = generate_seed_sequence(seed, 1)[0]

        if self._log:
            t_range = tqdm(range(nsamples),
                           desc=f"[Sampling progress]",
                           position=0,
                           leave=True,
                           colour='green')
        else:
            t_range = range(nsamples)

        # Config
        state = State(state.positions, state.logp, 0, state.delta)
        energies = np.zeros(nsamples)

        for i in t_range:
            state = self._step(state, v_bias, h_bias, kernel, seed_seq)
            energies[i] = self._rbm.local_energy(state.positions,
                                                 v_bias,
                                                 h_bias,
                                                 kernel
                                                 )

        energies = energies[burn:]
        energy = np.mean(energies)
        error = block(energies)
        variance = np.mean(energies**2) - energy**2
        acc_rate = state.n_accepted / (nsamples - burn)
        self._sampling_performed_ = True

        print("energy", energy)
        print("error", error)
        print("variance", variance)
        print("acc rate", acc_rate)

        return energies

    def _sample():
        """To be called by process"""
        pass

    def _rwm_step(self, state, v_bias, h_bias, kernel, seed):
        """One step of the random walk Metropolis algorithm

        Parameters
        ----------
        state : nqs.State
            Current state of the system. See state.py

        scale : float
            Scale of proposal distribution. Default: 0.5

        Returns
        -------
        new_state : nqs.State
            The updated state of the system.
        """

        # Advance RNG
        next_gen = advance_PRNG_state(seed, state.delta)
        rng = self._rng(next_gen)

        # Sample proposal positions, i.e., move walkers
        proposals = rng.normal(loc=state.positions, scale=self._scale)

        # Sample log uniform rvs
        log_unif = np.log(rng.random())

        # Compute proposal log density
        logp_proposal = self._rbm.logprob(proposals, v_bias, h_bias, kernel)

        # Metroplis acceptance criterion
        accept = log_unif < logp_proposal - state.logp

        # If accept is True, yield proposal, otherwise keep old state
        new_positions = proposals if accept else state.positions

        # Create new state
        new_logp = self._rbm.logprob(new_positions, v_bias, h_bias, kernel)
        new_n_accepted = state.n_accepted + accept
        new_delta = state.delta + 1
        new_state = State(new_positions, new_logp, new_n_accepted, new_delta)

        return new_state

    def _lmh_step(self, state, v_bias, h_bias, kernel, seed):
        """One step of the Langevin Metropolis-Hastings algorithm

        Parameters
        ----------
        state : State
            Current state of the system. See state.py
        alpha :
            Variational parameter
        D : float
            Diffusion constant. Default: 0.5
        dt : float
            Scale of proposal distribution. Default: 1.0
        """

        # Precompute
        dt = self._scale**2
        Ddt = 0.5 * dt
        quarterDdt = 1 / (4 * Ddt)
        sys_size = state.positions.shape

        # Advance RNG
        next_gen = advance_PRNG_state(seed, state.delta)
        rng = self._rng(next_gen)

        # Compute drift force at current positions
        F = self._rbm.drift_force(state.positions, v_bias, h_bias, kernel)

        # Sample proposal positions, i.e., move walkers
        proposals = state.positions + F * Ddt + \
            rng.normal(loc=0, scale=self._scale, size=sys_size)

        # Compute proposal log density
        logp_prop = self._rbm.logprob(proposals, v_bias, h_bias, kernel)

        # Green's function conditioned on proposals
        F_prop = self._rbm.drift_force(proposals, v_bias, h_bias, kernel)
        G_prop = -(state.positions - proposals - Ddt * F_prop)**2 * quarterDdt

        # Green's function conditioned on current positions
        G_cur = -(proposals - state.positions - Ddt * F)**2 * quarterDdt

        # Metroplis-Hastings ratio
        ratio = logp_prop + np.sum(G_prop) - state.logp - np.sum(G_cur)

        # Sample log uniform rvs
        log_unif = np.log(rng.random())

        # Metroplis acceptance criterion
        accept = log_unif < ratio

        # If accept is True, yield proposal, otherwise keep old state
        new_positions = proposals if accept else state.positions

        # Create new state
        new_logp = self._rbm.logprob(new_positions, v_bias, h_bias, kernel)
        new_n_accepted = state.n_accepted + accept
        new_delta = state.delta + 1
        new_state = State(new_positions, new_logp, new_n_accepted, new_delta)

        return new_state
