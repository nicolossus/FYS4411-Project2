%================================================================
\section{Methodology}\label{sec:Method}
%================================================================

%----------------------------------------------------------------
\subsection{Previous work}\label{sec:project method}
%----------------------------------------------------------------
This project is a continuation of our past work and will along with the theory presented above also include the methods presented in this section.

\subsection{Variational Monte Carlo}
% Former latter
Theory and description for both our Monte Carlo approaches, brute-force and importance sampling, can be accessed through our previous work \citep{project1}. The variational part of this project, i.e approximating the upper bound for the ground state energy of our system, will be based on our previous findings and analysis. In short, our results showed that the importance sampling implementation yielded the most accurate results and we will therefore use it as the basis four our VMC approach alongside the RBM. The importance sampling approach requires relatively few numbers of monte carlo cycles, $M$, for it to yield sufficient results, even for more complex systems than studied here. From our analysis, we noted that the optimal step size was $\Delta t = ?$

\subsection{Stochastic Gradient Decent}
%Our gradient decent approach can also be found in our previous work. We refrained from implementing a Stochastic Gradient Decent scheme as we simply do not have an abundance with datapoints to work with.

Our gradient decent approach can also be found in our previous work. In principle, we have as many datapoints as we want depending on what we want to simulate. We will therefore use Stochastic Gradient Decent (Optimization) approaches to find upper limit of numerically with different number of MC cycles. Although a SGD could provide us with decent results, it is not adequate enough for a variational problem depending on multiple parameters like in this project. We will the implement a momentumbased SGD and ADAM. 

Unlike previously, we will treat the local energy, $E_L$ as the cost and differentiate with regard to the variational parameter, $\alpha$. In order to obtain the wave function that is an eigenstate of the ground state energy, we will tune the $\alpha$ value containing the RBM parameters. In general, the premise is the same, except that we are working with multiple parameters, being visible and hidden weights and biases of the RBM. 

The gradient of the loss funciton, in our case the expectation value of the energy, with respect to an arbitrary parameter, $\bm{\lambda}$, of the RBM is given by
\begin{equation*}
    %C_i = \gradient \left \langle E_L(\mathbf{\alpha}_i) \right \rangle
    \grad_{\bm{\lambda}}\expval{E} = \grad_{\bm{\lambda}}\expval{E\qty(\bm{\lambda})}
\end{equation*}
which is found via \autoref{eq:update_rule}. 
For every step, displaying the standard gradient descent method, we update the value of $\bm{\lambda}_k$ by subtracting by the expectation value of the energy which then is multiplied by a learning rate, $\eta $ in the following way,

\begin{equation*}
    \bm{\lambda}_{k+1} =  \bm{\lambda}_k - \eta \grad_{\bm{\lambda}_k}\expval{E(\bm{\lambda}_k} \qquad \text{for} \,\, k \geq 0, 
\end{equation*}
where $\bm{\lambda}_k$ and $\bm{\lambda}_{k+1}$ represent the $k$th and ($k+1$)th values of $\bm{\lambda}$ respectively. This procedure is repeated until we get convergence (minimization in the cost function) for all trainable parameters.

\subsection{ADAM}
In addition of keeping track of the running average of the first momentum, the ADAM (ADAptive Momentestimation) algorithm also does the same for the second moment of the gradient. Complete description of this method can be found in our previous work \citep{project1}. 

% Endre navn på C_i

\subsection{Blocking}
In order to find the uncertainty in our sumulations, blocking will be used. The complete outline and mathematical backround of the blocking method used in our implementation, can be found in our previous work \citep{project1} and is based on Jonnson’s work \citep{MariusJonsson}. Error estimation and the blocking implementation here is the same as in our previous work. 


\subsection{VMC parameters and general outline}
We have in this project looked at two \textit{Markov Chain Monte Carlo} (MCMC) methods. The scale parameter \textit{Random Walk Metropolis} (RWM) and the \textit{Langevin Metropolis-Hastings} (LMH) algorithm where the former is a brute-force implementation and the latter uses importance sampling. Complete outline of these methods can be found in our previous work [cite p1]. For the single electron in one-dimension we found a suitable scale parameter for the RWM algorithm used in generating proposal states to be $3.0$, which yielded an acceptance ratio of $\sim 30\%$, while for the LMH algorithm we found the `time-step', scale$=\sqrt{dt}$, to be $1.3$ with an acceptance ratio of $\sim 60\%$. For the system of two interacting particles we used the scale parameters scale$=1.0$ for the RWM algorithm and $scale=1.0$ for the LMH algorithm which yielded, respectively, the acceptance ratios of $\sim 30\%$ and $\sim60\%$. 

% The scales of the proposal distributions, $\sigma_p$, are set to $\sigma_p=3.0$ and $\sigma_p=1.3$ for the RWM and LMH algorithms, respectively, which give acceptance rates of $\sim 30\%$ and $\sim60\%$, respectively. 