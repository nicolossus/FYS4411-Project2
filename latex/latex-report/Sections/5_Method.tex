%================================================================
\section{Methodology}\label{sec:Method}
%================================================================

%----------------------------------------------------------------
\subsection{Project Method 1}\label{sec:project method}
%----------------------------------------------------------------

\subsubsection{Variational Monte Carlo}

Theory and description of of both brute-force and the Metropolis-Hastings algorithm can be accessed through our previous work [cite p1] and we will therefore base our variational part on the on previous findings and analysis. In short, our results showed that the Metropolois-Hastings implementation yielded the most accurate results, we will use it as the basis for out Variational Monte Carlo (VMC) approach alongside the RBM. It requires relatively few numbers of Monte Carlo cycles , $M$, for it to yield sufficient results even for more complex systems than studied here. From our analysis, we noted that the optimal step size was $\Delta t = ?$ and 


\subsubsection{Optimization techniques}
%Our gradient decent approach can also be found in our previous work. We refrained from implementing a Stochastic Gradient Decent scheme as we simply do not have an abundance with datapoints to work with.

Our gradient decent approach can also be found in our previous work. In principle, we have as many datapoints as we want depending on what we want to simulate. We will therefore use Stochastic Gradient Decent (Optimization) approaches to find upper limit of numerically with a few monte carlo cycles. Although a SGD could provide us with decent results, it is not adequate enough for a variational problem depending on multiple parameters like in this project. We will the implement a momentumbased SGD and ADAM. 

Unlike previously, we will treat the local energy, $E_L$ as the cost and differentiate with regard to the variational parameter, $\alpha$. In order to obtain the wave function that is an eigenstate of the ground state energy, we will tune the $\alpha$ value containing the RBM parameters. In general, the premise is the same, except that we are working with multiple parameters, being visible and hidden weights and biases of the RBM. 

\begin{equation*}
    G_i = \nabla \left \langle E_L(\mathbf{\alpha}_i) \right \rangle
\end{equation*}

\subsubsection{ADAM}
In addition of keeping track of the running average of the first momentum, the ADAM (ADAptive Momentestimation) algorithm also does the same for the second moment of the gradient. Complete description of this method can be found in our previous work [cite p1]. 

