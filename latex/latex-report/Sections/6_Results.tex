%================================================================
\section{Results and Discussion}\label{sec:Results}
%================================================================

%----------------------------------------------------------------
\subsection{Non-Interacting}\label{sec:project results}
%----------------------------------------------------------------

\autoref{fig:train_iter_lr_batch1000} and \autoref{fig:train_iter_lr_batch5000} We vary the number of training iterations and the the learning rate, $\eta$. The training, or update of parameters, are done in batches of 5000 iterations, meaning that the x-axis tick labels correspond to the number of updates. 

Each point is the average of 8 Markov chains, each with expectation value of the energy, $\langle E \rangle$, the variance $\mathrm{Var}(E)$ and sampling error, $\sigma_b$, found via the blocking method, estimated from $2^{18}$ samples. The scales of the proposal distributions, $\sigma_p$, are set to $\sigma_p=3.0$ and $\sigma_p=1.3$ for the RWM and LMH algorithms, respectively, which give acceptance rates of $\sim 30\%$ and $\sim60\%$, respectively. 

System: 1 particle in 1 dimension, 2 hidden neurons, with the variance in the Gaussian layer set as $\sigma_\mathrm{RBM}^2=1.0$

\begin{figure}[!htb]
\begin{center}\includegraphics[width=\textwidth]{latex/figures/training_cycles_lr_batch1000.pdf}
\end{center}
\caption{c}
\label{fig:train_iter_lr_batch1000}
\end{figure}

\begin{figure}[!htb]
\begin{center}\includegraphics[width=\textwidth]{latex/figures/training_cycles_lr.pdf}
\end{center}
\caption{c}
\label{fig:train_iter_lr_batch5000}
\end{figure}


% Remove hidden_neurons_batch_size or have hidden_neurons_batch_size_extra in appendix?
\autoref{fig:hidden_neurons_batch_size}

\begin{figure}[!htb]
\begin{center}\includegraphics[width=\textwidth]{latex/figures/hidden_neurons_batch_size.pdf}
\end{center}
\caption{c}
\label{fig:hidden_neurons_batch_size}
\end{figure}


\autoref{fig:hidden_neurons_batch_size_extra}

\begin{figure}[!htb]
\begin{center}\includegraphics[width=\textwidth]{latex/figures/hidden_neurons_batch_size_extra.pdf}
\end{center}
\caption{c}
\label{fig:hidden_neurons_batch_size_extra}
\end{figure}

%----------------------------------------------------------------
\subsection{Interacting}\label{sec:project results}
%----------------------------------------------------------------














%Cost function - might be beneficial to use a cost function either based on minimizing the variance, Var(E), or on minimizing both $\langle E \rangle$ and Var(E). 

%The training itself is stochastic, and here we train the RBM parameters once, and then sample with the trained parameters with multiple Markov chains. A more rigorous approach would be to compare statistics across multiple training rounds with the same system and initial parameter settings

%Also train/optimize RBM scale